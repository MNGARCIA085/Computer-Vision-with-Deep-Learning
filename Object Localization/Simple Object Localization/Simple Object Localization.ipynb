{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f396c-aad3-4a50-8c4a-ef5f6e0fc77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3996e4a-6196-41e6-b417-a29c1f7324b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacc54e9-7902-4989-8104-4dff91bf838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import os, random\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"labels\", exist_ok=True)\n",
    "\n",
    "for i in range(1000):\n",
    "    img = Image.new(\"RGB\", (64, 64), (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    x, y = random.randint(10, 54), random.randint(10, 54)\n",
    "    size = 10\n",
    "    draw.rectangle([x-size, y-size, x+size, y+size], fill=(255, 0, 0))\n",
    "    img.save(f\"images/{i}.png\")\n",
    "    with open(f\"labels/{i}.txt\", \"w\") as f:\n",
    "        f.write(f\"{x},{y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e02f4b6-3a28-43ba-bcec-9e31ee883665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3_W1_Lab_1_transfer_learning_cats_dogs.ipynb  images\n",
      "C3_W1_Lab_2_Transfer_Learning_CIFAR_10.ipynb   labels\n",
      "C3_W1_Lab_3_Object_Localization.ipynb\t       Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f90bc7-ff7e-495b-87a3-2391149d8ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ea1ee-7855-4008-8a8a-53e987e3db5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133019f6-a226-41d2-8f33-1392d93bcdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f59c5-b468-433c-b760-bbf222914dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4eed9c-2ffb-4901-b5a5-c1f6988162a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second dataset so the classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b2f3ea12-722d-4282-a779-7caddea14e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Directories\n",
    "img_dir = \"shape_dataset/images\"\n",
    "ann_dir = \"shape_dataset/annotations\"\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "os.makedirs(ann_dir, exist_ok=True)\n",
    "\n",
    "# Image and shape parameters\n",
    "img_size = 64\n",
    "shape_size = 10\n",
    "num_images = 50  # Number of images to generate\n",
    "\n",
    "# Function to create bounding box XML\n",
    "def create_voc_xml(filename, width, height, bbox, shape_name, folder=\"images\"):\n",
    "    annotation = ET.Element(\"annotation\")\n",
    "    ET.SubElement(annotation, \"folder\").text = folder\n",
    "    ET.SubElement(annotation, \"filename\").text = filename\n",
    "    ET.SubElement(annotation, \"path\").text = os.path.join(folder, filename)\n",
    "\n",
    "    size = ET.SubElement(annotation, \"size\")\n",
    "    ET.SubElement(size, \"width\").text = str(width)\n",
    "    ET.SubElement(size, \"height\").text = str(height)\n",
    "    ET.SubElement(size, \"depth\").text = \"3\"\n",
    "\n",
    "    ET.SubElement(annotation, \"segmented\").text = \"0\"\n",
    "\n",
    "    obj = ET.SubElement(annotation, \"object\")\n",
    "    ET.SubElement(obj, \"name\").text = shape_name\n",
    "    ET.SubElement(obj, \"pose\").text = \"Unspecified\"\n",
    "    ET.SubElement(obj, \"truncated\").text = \"0\"\n",
    "    ET.SubElement(obj, \"difficult\").text = \"0\"\n",
    "\n",
    "    bndbox = ET.SubElement(obj, \"bndbox\")\n",
    "    ET.SubElement(bndbox, \"xmin\").text = str(bbox[0])\n",
    "    ET.SubElement(bndbox, \"ymin\").text = str(bbox[1])\n",
    "    ET.SubElement(bndbox, \"xmax\").text = str(bbox[2])\n",
    "    ET.SubElement(bndbox, \"ymax\").text = str(bbox[3])\n",
    "\n",
    "    return ET.ElementTree(annotation)\n",
    "\n",
    "# Function to draw shapes (circle or square) and return the bounding box\n",
    "def draw_shape(img_size, shape_type=\"square\"):\n",
    "    shape_size = 10\n",
    "    img = Image.new(\"RGB\", (img_size, img_size), (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    x, y = random.randint(shape_size, img_size - shape_size), random.randint(shape_size, img_size - shape_size)\n",
    "    xmin, ymin = x - shape_size, y - shape_size\n",
    "    xmax, ymax = x + shape_size, y + shape_size\n",
    "\n",
    "    if shape_type == \"square\":\n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], fill=(255, 0, 0))\n",
    "    elif shape_type == \"circle\":\n",
    "        draw.ellipse([xmin, ymin, xmax, ymax], fill=(0, 0, 255))\n",
    "\n",
    "    return img, (xmin, ymin, xmax, ymax), shape_type\n",
    "\n",
    "# Generate images and annotations\n",
    "for i in range(num_images):\n",
    "    shape_type = random.choice([\"square\", \"circle\"])\n",
    "    img, bbox, shape_name = draw_shape(img_size, shape_type)\n",
    "    img_filename = f\"{i:04d}.png\"\n",
    "    img.save(os.path.join(img_dir, img_filename))\n",
    "\n",
    "    xml_filename = f\"{i:04d}.xml\"\n",
    "    tree = create_voc_xml(img_filename, img_size, img_size, bbox, shape_name)\n",
    "    tree.write(os.path.join(ann_dir, xml_filename))\n",
    "\n",
    "# Function to draw bounding box on an image for display\n",
    "def draw_bbox(image_path, xml_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    bndbox = root.find(\".//bndbox\")\n",
    "    box = [int(bndbox.find(tag).text) for tag in [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]]\n",
    "    \n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.rectangle(box, outline=\"green\", width=2)\n",
    "    return img\n",
    "\n",
    "# Display a few images with bounding boxes\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i, fname in enumerate(sorted(os.listdir(img_dir))[:5]):\n",
    "    img_path = os.path.join(img_dir, fname)\n",
    "    xml_path = os.path.join(ann_dir, fname.replace(\".png\", \".xml\"))\n",
    "    img_with_bbox = draw_bbox(img_path, xml_path)\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(img_with_bbox)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3077406-26af-483c-abce-2a9ff1eb1045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d23019a8-3034-4c48-ab9a-82bd2eb3a18d",
   "metadata": {},
   "source": [
    "### Preparing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d42a71d-644a-460f-a7b4-3d3020811e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import random\n",
    "\n",
    "IMG_DIR = \"shape_dataset/images\"\n",
    "ANN_DIR = \"shape_dataset/annotations\"\n",
    "\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    label = root.find(\".//name\").text\n",
    "    label = 0 if label == \"square\" else 1  # square=0, circle=1\n",
    "\n",
    "    bbox = root.find(\".//bndbox\")\n",
    "    xmin = int(bbox.find(\"xmin\").text)\n",
    "    ymin = int(bbox.find(\"ymin\").text)\n",
    "    xmax = int(bbox.find(\"xmax\").text)\n",
    "    ymax = int(bbox.find(\"ymax\").text)\n",
    "\n",
    "    # Normalize bbox coordinates to [0, 1]\n",
    "    bbox_norm = [xmin / 64, ymin / 64, xmax / 64, ymax / 64]\n",
    "    return label, bbox_norm\n",
    "\n",
    "# Build dataset entries\n",
    "samples = []\n",
    "for fname in sorted(os.listdir(IMG_DIR)):\n",
    "    if fname.endswith(\".png\"):\n",
    "        img_path = os.path.join(IMG_DIR, fname)\n",
    "        xml_path = os.path.join(ANN_DIR, fname.replace(\".png\", \".xml\"))\n",
    "        label, bbox = parse_xml(xml_path)\n",
    "        samples.append((img_path, label, bbox))\n",
    "\n",
    "random.shuffle(samples)\n",
    "split_idx = int(0.8 * len(samples))\n",
    "train_samples = samples[:split_idx]\n",
    "val_samples = samples[split_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af5178a4-44a5-4647-b57c-3fb0c29ac889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_example(img_path, label, bbox):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)  # scale [0,1]\n",
    "    return img, {\"label\": label, \"bbox\": bbox}\n",
    "\n",
    "def make_dataset(samples, batch_size=4):\n",
    "    img_paths, labels, bboxes = zip(*samples)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((list(img_paths), list(labels), list(bboxes)))\n",
    "    ds = ds.map(lambda x, y, z: load_example(x, y, z))\n",
    "    ds = ds.shuffle(100).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(train_samples, batch_size=1)\n",
    "val_ds = make_dataset(val_samples, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa228a0-3a97-444b-ad18-db5ba1964364",
   "metadata": {},
   "source": [
    "train_ds and val_ds are tf.data.Dataset objects.\n",
    "\n",
    "Each item is a tuple:\n",
    "(image, {\"label\": shape_class, \"bbox\": normalized_box})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e06e0cb5-a84c-4fd3-b77a-6090c973c1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: (1, 64, 64, 3)\n",
      "Labels: tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "BBoxes: tf.Tensor([[0.40625  0.015625 0.71875  0.328125]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for images, targets in train_ds.take(1):\n",
    "    print(\"Images:\", images.shape)\n",
    "    print(\"Labels:\", targets[\"label\"])\n",
    "    print(\"BBoxes:\", targets[\"bbox\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae801a-80e6-43b9-acf8-da2be949466e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6675381-1dc1-4213-9326-58d0da025469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697d88f-3087-423a-b3c2-d582b4182d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54294f4e-5144-4ca1-ae80-a0bf47285007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 17:20:51.123856: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-07 17:20:51.123894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-07 17:20:51.125168: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-07 17:20:51.132014: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-07 17:20:53.032983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37526d7b-75d0-4747-a540-fea31838c8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)        [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 62, 62, 16)           448       ['input_7[0][0]']             \n",
      "                                                                                                  \n",
      " average_pooling2d_18 (Aver  (None, 31, 31, 16)           0         ['conv2d_18[0][0]']           \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 29, 29, 32)           4640      ['average_pooling2d_18[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_19 (Aver  (None, 14, 14, 32)           0         ['conv2d_19[0][0]']           \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 12, 12, 64)           18496     ['average_pooling2d_19[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_20 (Aver  (None, 6, 6, 64)             0         ['conv2d_20[0][0]']           \n",
      " agePooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)         (None, 2304)                 0         ['average_pooling2d_20[0][0]']\n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 128)                  295040    ['flatten_6[0][0]']           \n",
      "                                                                                                  \n",
      " classification (Dense)      (None, 2)                    258       ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " bounding_box (Dense)        (None, 4)                    516       ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 319398 (1.22 MB)\n",
      "Trainable params: 319398 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Feature extractor\n",
    "def feature_extractor(inputs):\n",
    "    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu')(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=3, activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "    return x\n",
    "\n",
    "# Dense layers\n",
    "def dense_layers(inputs):\n",
    "    x = tf.keras.layers.Flatten()(inputs)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "# Classification branch (2 classes)\n",
    "def classifier(inputs):\n",
    "    return tf.keras.layers.Dense(2, activation='softmax', name='classification')(inputs)\n",
    "\n",
    "# Bounding box regression branch (4 values)\n",
    "def bounding_box_regression(inputs):\n",
    "    return tf.keras.layers.Dense(4, name='bounding_box')(inputs)\n",
    "\n",
    "# Complete model\n",
    "def final_model(inputs):\n",
    "    features = feature_extractor(inputs)\n",
    "    dense = dense_layers(features)\n",
    "    class_out = classifier(dense)\n",
    "    bbox_out = bounding_box_regression(dense)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=[class_out, bbox_out])\n",
    "\n",
    "# Compile model\n",
    "def define_and_compile_model(input_shape=(64, 64, 3)):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    model = final_model(inputs)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={\n",
    "            'classification': 'sparse_categorical_crossentropy',\n",
    "            'bounding_box': 'mse'\n",
    "        },\n",
    "        metrics={\n",
    "            'classification': 'accuracy',\n",
    "            'bounding_box': 'mse'\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize\n",
    "model = define_and_compile_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2dcbe1-575d-4bb5-9193-ee708969e32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d510f-fdc4-4d78-9e4a-2d9781e873ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "765de788-6625-4608-91df-e2ad3a1a2f7b",
   "metadata": {},
   "source": [
    "Each item muste be:\n",
    "\n",
    "(image_tensor, {\"classification\": int_label, \"bounding_box\": [xmin, ymin, xmax, ymax]})\n",
    "\n",
    "\n",
    "\n",
    "not:\n",
    "\n",
    "(image_tensor, {\"label\": ..., \"bbox\": ...})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bd7b3-b2a6-41f1-88e2-199fabf93612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6bb8c636-d055-401c-b223-1d7434626bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_keys(img, targets):\n",
    "    return img, {\n",
    "        \"classification\": targets[\"label\"],\n",
    "        \"bounding_box\": targets[\"bbox\"]\n",
    "    }\n",
    "\n",
    "train_ds = train_ds.map(remap_keys)\n",
    "val_ds = val_ds.map(remap_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "15ecd86d-6a3d-4f99-a0cb-9ae4529d2cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 1s 11ms/step - loss: 0.7499 - classification_loss: 0.6780 - bounding_box_loss: 0.0719 - classification_accuracy: 0.6750 - bounding_box_mse: 0.0719 - val_loss: 0.5273 - val_classification_loss: 0.4287 - val_bounding_box_loss: 0.0985 - val_classification_accuracy: 0.8000 - val_bounding_box_mse: 0.0985\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2762 - classification_loss: 0.2134 - bounding_box_loss: 0.0628 - classification_accuracy: 0.9500 - bounding_box_mse: 0.0628 - val_loss: 0.1515 - val_classification_loss: 0.0797 - val_bounding_box_loss: 0.0718 - val_classification_accuracy: 1.0000 - val_bounding_box_mse: 0.0718\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1463 - classification_loss: 0.0488 - bounding_box_loss: 0.0975 - classification_accuracy: 1.0000 - bounding_box_mse: 0.0975 - val_loss: 0.1062 - val_classification_loss: 0.0561 - val_bounding_box_loss: 0.0501 - val_classification_accuracy: 1.0000 - val_bounding_box_mse: 0.0501\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1235 - classification_loss: 0.0454 - bounding_box_loss: 0.0781 - classification_accuracy: 1.0000 - bounding_box_mse: 0.0781 - val_loss: 0.0669 - val_classification_loss: 0.0232 - val_bounding_box_loss: 0.0436 - val_classification_accuracy: 1.0000 - val_bounding_box_mse: 0.0436\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.0660 - classification_loss: 0.0171 - bounding_box_loss: 0.0490 - classification_accuracy: 1.0000 - bounding_box_mse: 0.0490 - val_loss: 0.0326 - val_classification_loss: 0.0165 - val_bounding_box_loss: 0.0160 - val_classification_accuracy: 1.0000 - val_bounding_box_mse: 0.0160\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.0201 - classification_loss: 0.0075 - bounding_box_loss: 0.0126 - classification_accuracy: 1.0000 - bounding_box_mse: 0.0126 - val_loss: 0.0126 - val_classification_loss: 0.0062 - val_bounding_box_loss: 0.0064 - val_classification_accuracy: 1.0000 - val_bounding_box_mse: 0.0064\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.0107 - classification_loss: 0.0035 - bounding_box_loss: 0.0071 - classification_accuracy: 1.0000 - bounding_box_mse: 0.0071 - val_loss: 0.0149 - val_classification_loss: 0.0039 - val_bounding_box_loss: 0.0110 - val_classification_accuracy: 1.0000 - val_bounding_box_mse: 0.0110\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.0086 - classification_loss: 0.0025 - bounding_box_loss: 0.0061 - classification_accuracy: 1.0000 - bounding_box_mse: 0.0061 - val_loss: 0.0079 - val_classification_loss: 0.0026 - val_bounding_box_loss: 0.0052 - val_classification_accuracy: 1.0000 - val_bounding_box_mse: 0.0052\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.0058 - classification_loss: 0.0015 - bounding_box_loss: 0.0043 - classification_accuracy: 1.0000 - bounding_box_mse: 0.0043 - val_loss: 0.0182 - val_classification_loss: 0.0019 - val_bounding_box_loss: 0.0163 - val_classification_accuracy: 1.0000 - val_bounding_box_mse: 0.0163\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.0098 - classification_loss: 0.0014 - bounding_box_loss: 0.0084 - classification_accuracy: 1.0000 - bounding_box_mse: 0.0084 - val_loss: 0.0075 - val_classification_loss: 0.0023 - val_bounding_box_loss: 0.0052 - val_classification_accuracy: 1.0000 - val_bounding_box_mse: 0.0052\n"
     ]
    }
   ],
   "source": [
    "model = define_and_compile_model()\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630381b-38b2-4f7c-9dfe-6375fa907814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ddd8f5-517e-4c52-907d-553108fc99a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c12568f-ab23-420a-8aaf-18ea63fccb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0334aec9-3931-4352-9729-5942f5abba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be647b2b-8cd9-44de-8de3-5d90e45aac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_and_compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8c7c9f2-47e2-491c-9a4f-e6affd6e20e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 1s 30ms/step - loss: 0.8083 - classification_loss: 0.7296 - bounding_box_loss: 0.0787 - classification_accuracy: 0.6000 - bounding_box_mse: 0.0787 - val_loss: 0.7675 - val_classification_loss: 0.6720 - val_bounding_box_loss: 0.0955 - val_classification_accuracy: 0.5000 - val_bounding_box_mse: 0.0955\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.6884 - classification_loss: 0.6281 - bounding_box_loss: 0.0603 - classification_accuracy: 0.6000 - bounding_box_mse: 0.0603 - val_loss: 0.7113 - val_classification_loss: 0.6368 - val_bounding_box_loss: 0.0745 - val_classification_accuracy: 0.5000 - val_bounding_box_mse: 0.0745\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.5475 - classification_loss: 0.4940 - bounding_box_loss: 0.0536 - classification_accuracy: 0.6500 - bounding_box_mse: 0.0536 - val_loss: 0.4866 - val_classification_loss: 0.4173 - val_bounding_box_loss: 0.0693 - val_classification_accuracy: 0.5000 - val_bounding_box_mse: 0.0693\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.2819 - classification_loss: 0.2257 - bounding_box_loss: 0.0562 - classification_accuracy: 0.9750 - bounding_box_mse: 0.0562 - val_loss: 0.2021 - val_classification_loss: 0.1325 - val_bounding_box_loss: 0.0696 - val_classification_accuracy: 1.0000 - val_bounding_box_mse: 0.0696\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.1730 - classification_loss: 0.0891 - bounding_box_loss: 0.0839 - classification_accuracy: 0.9750 - bounding_box_mse: 0.0839 - val_loss: 0.0810 - val_classification_loss: 0.0431 - val_bounding_box_loss: 0.0380 - val_classification_accuracy: 1.0000 - val_bounding_box_mse: 0.0380\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0884 - classification_loss: 0.0408 - bounding_box_loss: 0.0477 - classification_accuracy: 1.0000 - bounding_box_mse: 0.0477\n",
      "Validation accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5 # 45\n",
    "#steps_per_epoch = 60000//BATCH_SIZE  # 60,000 items in this dataset\n",
    "#validation_steps = 1\n",
    "\n",
    "training_dataset = train_ds\n",
    "validation_dataset = val_ds\n",
    "\n",
    "\n",
    "history = model.fit(training_dataset,\n",
    "                    validation_data=validation_dataset, epochs=EPOCHS)\n",
    "\n",
    "loss, classification_loss, bounding_box_loss, classification_accuracy, bounding_box_mse = model.evaluate(validation_dataset, steps=1)\n",
    "print(\"Validation accuracy: \", classification_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac5b1ec-2482-40e7-abb0-aa014370a3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded627fe-6b61-4256-a4a7-d406b2d60b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5644fc-adec-4083-94d3-259c3a270f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df5ba4-323d-400c-85b5-53952d6b0440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "630bf20b-b679-49cd-a7ef-15bd1bb076e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(pred_box, true_box):\n",
    "    xmin_pred, ymin_pred, xmax_pred, ymax_pred =  np.split(pred_box, 4, axis = 1)\n",
    "    xmin_true, ymin_true, xmax_true, ymax_true = np.split(true_box, 4, axis = 1)\n",
    "\n",
    "    smoothing_factor = 1e-10\n",
    "\n",
    "    xmin_overlap = np.maximum(xmin_pred, xmin_true)\n",
    "    xmax_overlap = np.minimum(xmax_pred, xmax_true)\n",
    "    ymin_overlap = np.maximum(ymin_pred, ymin_true)\n",
    "    ymax_overlap = np.minimum(ymax_pred, ymax_true)\n",
    "\n",
    "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
    "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
    "\n",
    "    overlap_area = np.maximum((xmax_overlap - xmin_overlap), 0)  * np.maximum((ymax_overlap - ymin_overlap), 0)\n",
    "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
    "\n",
    "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14630b4c-f54b-4ac1-ba79-5d320bfefb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9fb3f4f8-9c37-42b0-838b-d54a3a68881d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
       " 'bounding_box': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.484375, 0.578125, 0.796875, 0.890625]], dtype=float32)>}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(val_ds))\n",
    "\n",
    "label_batch['bounding_box']\n",
    "\n",
    "label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19a465ea-6dd5-4305-87e3-72194f84c552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 64, 3), dtype=float32, numpy=\n",
       "array([[[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e88fb050-9d9e-4544-b87d-fa0067af82fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "[1] [[0.05348994 0.54890233 0.44089186 0.84774405]]\n",
      "label {'classification': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, 'bounding_box': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.125   , 0.515625, 0.4375  , 0.828125]], dtype=float32)>}\n",
      "[0.05348994 0.54890233 0.44089186 0.84774405]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get one batch from the val_ds\n",
    "image_batch, label_batch = next(iter(val_ds))\n",
    "\n",
    "# Get the first image from the batch (assuming the batch size is > 1)\n",
    "image = image_batch[0]\n",
    "label = label_batch #[0]\n",
    "\n",
    "# Make a prediction using the model\n",
    "predictions = model.predict(tf.expand_dims(image, axis=0))  # Add batch dimension\n",
    "\n",
    "# Assuming the predictions are in two parts (classification and bounding box)\n",
    "predicted_class = np.argmax(predictions[0], axis=1)  # Classification prediction\n",
    "predicted_bbox = predictions[1]  # Bounding box prediction\n",
    "\n",
    "\n",
    "print(predicted_class, predicted_bbox)\n",
    "\n",
    "print('label',label)\n",
    "\n",
    "\n",
    "# Display the image with the predicted bounding box\n",
    "def plot_image_with_bbox(image, predicted_class, predicted_bbox):\n",
    "    plt.imshow(image)\n",
    "    xmin, ymin, xmax, ymax = predicted_bbox[0]\n",
    "    \n",
    "    \n",
    "    print(predicted_bbox[0])\n",
    "    \n",
    "    plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, \n",
    "                                      edgecolor='violet', facecolor='none', lw=2))\n",
    "    plt.title(f\"Predicted Class: {'Circle' if predicted_class == 1 else 'Square'}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Plot the image and bounding box\n",
    "plot_image_with_bbox(image, predicted_class, predicted_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8782c2-d336-4a26-9714-4eaa67d68a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "de6fbc4d-80d2-4c61-ac3c-cf8c58ddb855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "Predicted class: [1]\n",
      "Predicted bbox: [[0.46252578 0.5886995  0.85547346 0.90397805]]\n",
      "Label: {'classification': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, 'bounding_box': <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.484375, 0.578125, 0.796875, 0.890625]], dtype=float32)>}\n",
      "Scaled Bounding Box: [29.60165023803711, 37.6767692565918, 54.750301361083984, 57.85459518432617]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get one batch from the val_ds\n",
    "image_batch, label_batch = next(iter(val_ds))\n",
    "\n",
    "# Get the first image from the batch (assuming the batch size is > 1)\n",
    "image = image_batch[0]\n",
    "label = label_batch #[0]\n",
    "\n",
    "# Make a prediction using the model\n",
    "predictions = model.predict(tf.expand_dims(image, axis=0))  # Add batch dimension\n",
    "\n",
    "# Assuming the predictions are in two parts (classification and bounding box)\n",
    "predicted_class = np.argmax(predictions[0], axis=1)  # Classification prediction\n",
    "predicted_bbox = predictions[1]  # Bounding box prediction\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)\n",
    "print(\"Predicted bbox:\", predicted_bbox)\n",
    "\n",
    "print(\"Label:\", label)\n",
    "\n",
    "# Display the image with the predicted bounding box\n",
    "def plot_image_with_bbox(image, predicted_class, predicted_bbox):\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Assuming the bounding box is in normalized coordinates (0-1), scale to image size\n",
    "    xmin, ymin, xmax, ymax = predicted_bbox[0]  # Get the first bounding box\n",
    "    \n",
    "    # Scale the coordinates to the image size (64x64)\n",
    "    xmin, ymin, xmax, ymax = xmin * 64, ymin * 64, xmax * 64, ymax * 64\n",
    "    \n",
    "    print(\"Scaled Bounding Box:\", [xmin, ymin, xmax, ymax])\n",
    "    \n",
    "    # Plot the bounding box on the image\n",
    "    plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, \n",
    "                                      edgecolor='violet', facecolor='none', lw=2))\n",
    "    plt.title(f\"Predicted Class: {'Circle' if predicted_class == 1 else 'Square'}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Plot the image and bounding box\n",
    "plot_image_with_bbox(image, predicted_class, predicted_bbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698831b-69bc-4a67-a2f5-655cdb4f7522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af30137d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04036ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eda4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490aab84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "07ed986f-3d00-4d23-b0f7-308839af120b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.484375, 0.578125, 0.796875, 0.890625]], dtype=float32)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label['bounding_box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f823ffc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.484375, 0.578125, 0.796875, 0.890625], dtype=float32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label['bounding_box'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24f95654-a289-448a-9dcf-e34fa83a5456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46252578, 0.5886995 , 0.85547346, 0.90397805], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_bbox[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca69cc7-b28f-4fe6-a873-2625e9e3bd09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b3b3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ca546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11f530c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(pred_box, true_box):\n",
    "    #xmin_pred, ymin_pred, xmax_pred, ymax_pred =  np.split(pred_box, 4, axis = 1)\n",
    "    #xmin_true, ymin_true, xmax_true, ymax_true = np.split(true_box, 4, axis = 1)\n",
    "    xmin_pred, ymin_pred, xmax_pred, ymax_pred = pred_box\n",
    "    xmin_true, ymin_true, xmax_true, ymax_true = true_box\n",
    "    \n",
    "\n",
    "    smoothing_factor = 1e-10\n",
    "\n",
    "    xmin_overlap = np.maximum(xmin_pred, xmin_true)\n",
    "    xmax_overlap = np.minimum(xmax_pred, xmax_true)\n",
    "    ymin_overlap = np.maximum(ymin_pred, ymin_true)\n",
    "    ymax_overlap = np.minimum(ymax_pred, ymax_true)\n",
    "\n",
    "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
    "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
    "\n",
    "    overlap_area = np.maximum((xmax_overlap - xmin_overlap), 0)  * np.maximum((ymax_overlap - ymin_overlap), 0)\n",
    "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
    "\n",
    "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2c559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20df11b9-2c0b-416f-bbf3-023ce8f6c500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.7418025>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection_over_union(predicted_bbox[0], label['bounding_box'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b5f86-354d-49aa-b7f7-e9433ccfce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kodeco.com/books/machine-learning-by-tutorials/v2.0/chapters/9-beyond-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f6c93-672c-4c5e-80e4-22c590980c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7dbe8-ce00-4d84-9a42-47337aa0dfa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9539008-0930-4aba-973e-3bd5a6b6b948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78de70-468a-41d7-beb9-ad9720f9627b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
