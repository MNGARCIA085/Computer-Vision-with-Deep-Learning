{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18659af4-9392-452c-862d-43179a629407",
   "metadata": {},
   "source": [
    "# <b> <center> <font color='#7B241C'> OBJECT DETECTION - MULTIPLE OBJECTS </font> </center> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9416852-5c53-4f48-aae1-fa8c850c146c",
   "metadata": {},
   "source": [
    "## <font color='blue'> Table of Contents </font>\n",
    "\n",
    "1. [Introduction](#1)\n",
    "2. [Setup](#2)\n",
    "3. [Helper Functions](#3)\n",
    "4. [Data Generation and Pre-processing](#4)\n",
    "5. [Build, compile and train the model](#5)\n",
    "6. [Making predictions](#6)\n",
    "7. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1bc174-8566-4cd2-a889-058ee4e5b976",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## <b> <font color='red'> 1. Introduction </font> </b>\n",
    "\n",
    "In this notebook, we train an object detection model capable of identifying and localizing multiple objects within the same image. Unlike simple classification tasks, object detection involves predicting both the class and bounding box for each object present.\n",
    "\n",
    "To simplify data acquisition and maintain control over object placement and variety, we use synthetic data—artificially generated images containing different shapes (e.g., circles, squares) positioned at random locations. This allows us to simulate real-world conditions where multiple distinct objects can appear simultaneously in a single frame.\n",
    "\n",
    "The main goals of this notebook are:\n",
    "\n",
    "Generate a synthetic dataset with labeled bounding boxes and classes for multiple objects per image.\n",
    "\n",
    "Build and train an object detection model.\n",
    "\n",
    "Evaluate the model’s ability to detect and localize each object in new images.\n",
    "\n",
    "This project is a foundational step for tasks like automated visual inspection, multi-object tracking, and real-time detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032e93f-b570-4bfe-bbe2-19d6becdf472",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## <b> <font color='red'> 2. Setup </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c51f9f4-cc98-4e2c-8f49-762293f19bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1816ce72-b64f-4a44-97d1-50a403645f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90108a0c-92de-4e87-8b84-9caa1daa87a9",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## <b> <font color='red'> 3. Helper Functions </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "327b8607-3f48-4a56-ae38-da74d314aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(img_array, labels, bboxes):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img_array)\n",
    "\n",
    "    class_names = [\"square\", \"circle\"]\n",
    "    colors = [\"violet\", \"yellow\"]\n",
    "\n",
    "    for cls in range(len(labels)):\n",
    "        if labels[cls]:  # if object is present\n",
    "            x1, y1, x2, y2 = bboxes[cls] * IMG_SIZE  # denormalize\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            rect = patches.Rectangle((x1, y1), width, height,\n",
    "                                     linewidth=2, edgecolor=colors[cls],\n",
    "                                     facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1 - 5, class_names[cls], color=colors[cls], fontsize=12)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351497aa-8051-4814-99ac-08f3b66bec86",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## <b> <font color='red'> 4. Data Generation and Pre-processing </font> </b>\n",
    "\n",
    "We will generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0043603c-e372-472a-a34e-cccc173df088",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "NUM_CLASSES = 2  # red square, blue circle\n",
    "\n",
    "# ==== DATA GENERATION ====\n",
    "def generate_sample():\n",
    "    img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), \"black\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    labels = np.zeros(NUM_CLASSES)\n",
    "    bboxes = np.zeros((NUM_CLASSES, 4))  # x1, y1, x2, y2\n",
    "\n",
    "    for cls in range(NUM_CLASSES):\n",
    "        x1, y1 = random.randint(5, 30), random.randint(5, 30)\n",
    "        x2, y2 = x1 + 20, y1 + 20\n",
    "\n",
    "        if cls == 0: draw.rectangle([x1, y1, x2, y2], fill=\"red\")\n",
    "        if cls == 1: draw.ellipse([x1, y1, x2, y2], fill=\"blue\")\n",
    "\n",
    "        labels[cls] = 1\n",
    "        bboxes[cls] = np.array([x1, y1, x2, y2]) / IMG_SIZE  # normalize\n",
    "\n",
    "    img_array = np.array(img) / 255.0\n",
    "    return img_array, labels, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1937ffdc-3f6d-456a-be2d-7d177f8ba4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "img_array, labels, bboxes = generate_sample()\n",
    "plot_sample(img_array, labels, bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a57a545-e4ea-498d-b651-d4a238cdca57",
   "metadata": {},
   "source": [
    "Let's create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e1d443-dfab-4ab9-907c-50231876d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(num_samples=1000):\n",
    "    X, y_cls, y_bbox = [], [], []\n",
    "    for _ in range(num_samples):\n",
    "        img, labels, bboxes = generate_sample()\n",
    "        X.append(img)\n",
    "        y_cls.append(labels)\n",
    "        y_bbox.append(bboxes)\n",
    "    return np.array(X), np.array(y_cls), np.array(y_bbox)\n",
    "\n",
    "X_train, y_train_cls, y_train_bbox = create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a18284-1e3b-4aaa-8eae-c81b85916c87",
   "metadata": {},
   "source": [
    "Now, let's create the TF Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba65a004-6423-4899-9b68-1a49ad853321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and validation sets\n",
    "X_train, y_train_cls, y_train_bbox = create_dataset(num_samples=1000)\n",
    "X_val, y_val_cls, y_val_bbox = create_dataset(num_samples=200)\n",
    "\n",
    "# Create TensorFlow Datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train, {\"cls\": y_train_cls, \"bbox\": y_train_bbox})\n",
    ")\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_val, {\"cls\": y_val_cls, \"bbox\": y_val_bbox})\n",
    ")\n",
    "\n",
    "# Shuffle, batch, and prefetch\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = (\n",
    "    train_ds.shuffle(buffer_size=len(X_train))\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# no shuffle\n",
    "val_ds = (\n",
    "    val_ds.batch(BATCH_SIZE)\n",
    "          .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c63c18-c02e-43e5-8542-80720c09e157",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## <b> <font color='red'> 5. Build, compile and train the model </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a72c429-5f15-4598-9c27-3469b0cb8b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 64, 64, 32)           896       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 32, 32, 32)           0         ['conv2d[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)           18496     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 16, 16, 64)           0         ['conv2d_1[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 16384)                0         ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  2097280   ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 8)                    1032      ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " cls (Dense)                 (None, 2)                    258       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " bbox (Reshape)              (None, 2, 4)                 0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2117962 (8.08 MB)\n",
      "Trainable params: 2117962 (8.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_simple_model(img_size=64, num_classes=2):\n",
    "    inputs = layers.Input(shape=(img_size, img_size, 3))\n",
    "\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    # Output for classification: multi-label (presence of each class)\n",
    "    cls_output = layers.Dense(num_classes, activation='sigmoid', name='cls')(x)\n",
    "\n",
    "    # Output for bounding boxes: 4 coordinates per class (x1, y1, x2, y2)\n",
    "    bbox_output = layers.Dense(num_classes * 4, activation='sigmoid')(x)\n",
    "    bbox_output = layers.Reshape((num_classes, 4), name='bbox')(bbox_output)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[cls_output, bbox_output])\n",
    "    return model\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2 # circle and square\n",
    "\n",
    "# Example usage\n",
    "model = create_simple_model(img_size=IMG_SIZE, num_classes=NUM_CLASSES)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c7de84-6df8-4f64-aff4-80c13cd743da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 4s 84ms/step - loss: 0.0542 - cls_loss: 0.0317 - bbox_loss: 0.0225 - cls_binary_accuracy: 0.9865 - bbox_mean_absolute_error: 0.1076 - val_loss: 0.0033 - val_cls_loss: 6.7335e-05 - val_bbox_loss: 0.0033 - val_cls_binary_accuracy: 1.0000 - val_bbox_mean_absolute_error: 0.0452\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 3s 79ms/step - loss: 0.0013 - cls_loss: 8.0293e-05 - bbox_loss: 0.0012 - cls_binary_accuracy: 1.0000 - bbox_mean_absolute_error: 0.0264 - val_loss: 7.3242e-04 - val_cls_loss: 7.4082e-05 - val_bbox_loss: 6.5834e-04 - val_cls_binary_accuracy: 1.0000 - val_bbox_mean_absolute_error: 0.0193\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 4.7393e-04 - cls_loss: 4.9340e-05 - bbox_loss: 4.2459e-04 - cls_binary_accuracy: 1.0000 - bbox_mean_absolute_error: 0.0157 - val_loss: 4.4110e-04 - val_cls_loss: 3.5815e-05 - val_bbox_loss: 4.0529e-04 - val_cls_binary_accuracy: 1.0000 - val_bbox_mean_absolute_error: 0.0147\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 3.1248e-04 - cls_loss: 2.6354e-05 - bbox_loss: 2.8613e-04 - cls_binary_accuracy: 1.0000 - bbox_mean_absolute_error: 0.0126 - val_loss: 3.4191e-04 - val_cls_loss: 2.1080e-05 - val_bbox_loss: 3.2083e-04 - val_cls_binary_accuracy: 1.0000 - val_bbox_mean_absolute_error: 0.0127\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 2.3954e-04 - cls_loss: 1.5707e-05 - bbox_loss: 2.2383e-04 - cls_binary_accuracy: 1.0000 - bbox_mean_absolute_error: 0.0109 - val_loss: 2.7961e-04 - val_cls_loss: 1.3086e-05 - val_bbox_loss: 2.6652e-04 - val_cls_binary_accuracy: 1.0000 - val_bbox_mean_absolute_error: 0.0117\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 1.8705e-04 - cls_loss: 1.0318e-05 - bbox_loss: 1.7673e-04 - cls_binary_accuracy: 1.0000 - bbox_mean_absolute_error: 0.0097 - val_loss: 2.3263e-04 - val_cls_loss: 8.8215e-06 - val_bbox_loss: 2.2381e-04 - val_cls_binary_accuracy: 1.0000 - val_bbox_mean_absolute_error: 0.0103\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 1.5192e-04 - cls_loss: 7.1899e-06 - bbox_loss: 1.4473e-04 - cls_binary_accuracy: 1.0000 - bbox_mean_absolute_error: 0.0087 - val_loss: 2.0437e-04 - val_cls_loss: 6.4022e-06 - val_bbox_loss: 1.9797e-04 - val_cls_binary_accuracy: 1.0000 - val_bbox_mean_absolute_error: 0.0100\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 1.3282e-04 - cls_loss: 5.3985e-06 - bbox_loss: 1.2742e-04 - cls_binary_accuracy: 1.0000 - bbox_mean_absolute_error: 0.0081 - val_loss: 1.8600e-04 - val_cls_loss: 4.9786e-06 - val_bbox_loss: 1.8102e-04 - val_cls_binary_accuracy: 1.0000 - val_bbox_mean_absolute_error: 0.0093\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 1.1623e-04 - cls_loss: 4.2696e-06 - bbox_loss: 1.1196e-04 - cls_binary_accuracy: 1.0000 - bbox_mean_absolute_error: 0.0076 - val_loss: 1.6426e-04 - val_cls_loss: 3.9911e-06 - val_bbox_loss: 1.6027e-04 - val_cls_binary_accuracy: 1.0000 - val_bbox_mean_absolute_error: 0.0086\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 1.0789e-04 - cls_loss: 3.5168e-06 - bbox_loss: 1.0437e-04 - cls_binary_accuracy: 1.0000 - bbox_mean_absolute_error: 0.0074 - val_loss: 1.5603e-04 - val_cls_loss: 3.4832e-06 - val_bbox_loss: 1.5255e-04 - val_cls_binary_accuracy: 1.0000 - val_bbox_mean_absolute_error: 0.0083\n"
     ]
    }
   ],
   "source": [
    "# Compile example (adjust loss and metrics as needed)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        \"cls\": tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        \"bbox\": tf.keras.losses.MeanSquaredError()\n",
    "    },\n",
    "    metrics={\n",
    "        \"cls\": tf.keras.metrics.BinaryAccuracy(),\n",
    "        \"bbox\": tf.keras.metrics.MeanAbsoluteError()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Fit the model using the datasets created before\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf2e2b-96f7-4688-a337-3c5022f2555d",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## <b> <font color='red'> 6. Making predictions </font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419fe5d-c6eb-4978-9dd1-2b50137466d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = X_val[7]\n",
    "pred_cls, pred_bbox = model.predict(np.expand_dims(img, 0), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05071f08-54d2-48cf-8457-e05209577420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999833, 0.9999991 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9592bc3-6a25-43ac-b537-72307966fcd5",
   "metadata": {},
   "source": [
    "**Classification output**\n",
    "\n",
    "Shape: (1, 2) — 1 sample, 2 classes.\n",
    "\n",
    "Each value is between 0 and 1, representing the predicted probability (confidence) that each class is present in the image.\n",
    "\n",
    "In this example:\n",
    "\n",
    "- Class 0 (e.g., square) confidence ≈ 0.99999833 → very confident the object is present.\n",
    "\n",
    "- Class 1 (e.g., circle) confidence ≈ 0.9999991 → very confident the object is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4760679b-1f2c-4dd9-9acf-e8d0f665b950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.47013748, 0.2053786 , 0.7740922 , 0.5240758 ],\n",
       "        [0.19972506, 0.18141209, 0.5215569 , 0.4973964 ]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21696e8a-1ad2-4807-af60-0a742e8a6b2e",
   "metadata": {},
   "source": [
    "**Bounding Box Output**\n",
    "\n",
    "Shape: (1, 2, 4) — 1 sample, 2 classes, 4 coordinates per box.\n",
    "\n",
    "Each bounding box is represented as [x1, y1, x2, y2], normalized between 0 and 1 relative to image size.\n",
    "\n",
    "- For class 0 (square):\n",
    "    - Bounding box: [0.4701, 0.2053, 0.7741, 0.5241] <br> <br>\n",
    "\n",
    "\n",
    "- For class 1 (circle):\n",
    "    - Bounding box: [0.1997, 0.1814, 0.5216, 0.4974]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea81e910-8e0f-4f03-8d9a-ab4c0d308f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== INFERENCE EXAMPLE ====\n",
    "def show_prediction(model, X, idx=0):\n",
    "    img = X[idx]\n",
    "    pred_cls, pred_bbox = model.predict(np.expand_dims(img, 0), verbose=0)\n",
    "    pred_cls, pred_bbox = pred_cls[0], pred_bbox[0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    class_names = [\"Red Square\", \"Blue Circle\"]\n",
    "    colors = ['violet', 'yellow']\n",
    "\n",
    "    for i in range(NUM_CLASSES):\n",
    "        confidence = pred_cls[i]\n",
    "        if confidence > 0.5:\n",
    "            x1, y1, x2, y2 = pred_bbox[i] * IMG_SIZE\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            rect = plt.Rectangle((x1, y1), width, height,\n",
    "                                 edgecolor=colors[i], facecolor='none', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1 - 2, f\"{class_names[i]} ({confidence:.2f})\",\n",
    "                    color=colors[i], fontsize=8, backgroundcolor='black')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f866402-cb07-42de-a23b-be601e7074b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_prediction(model, X_val, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee4499-ead4-4177-8ffe-62ec8839e491",
   "metadata": {},
   "source": [
    "<a name=\"references\"></a>\n",
    "## <b> <font color='red'> References </font>  </b>\n",
    "\n",
    "[Advanced Computer Vision with TF](https://www.coursera.org/learn/advanced-computer-vision-with-tensorflow?specialization=tensorflow-advanced-techniques)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
