{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d545d3c7-39a5-46f8-a165-b25a1e23947b",
   "metadata": {},
   "source": [
    "# <center><div style=\"color:red\">PRE-PROCESSING: SIMPLE DATA</div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778ebcc-e85b-47ea-9161-8fa2347e5b34",
   "metadata": {},
   "source": [
    "### <font color='blue'> Table of Contents </font>\n",
    "- [1 - Objectives](#1)\n",
    "- [2 - Setup](#2)\n",
    "- [3 - Helper Functions](#3)\n",
    "- [4 - Download Data](#4)\n",
    "- [5 - Understanding the Data](#5)\n",
    "- [6 - Image and Mask Pre-processing](#6) <br>\n",
    "    - [6.1. - Image Pre-processing](#6) <br>\n",
    "    - [6.2. - Mask Pre-processing](#6) <br>\n",
    "    - [6.3. - Putting everything together](#6) <br>\n",
    "- [7 - Creating TF Datasets](#7)\n",
    "- [8 - References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d66ecc-0f57-4bf0-9eff-e0576cfb4bbe",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a> \n",
    "## <font color=\"orange\"> <b> 1. Introduction </b> </font>\n",
    "\n",
    "In this notebook, we demonstrate how to perform preprocessing for image segmentation tasks using simple synthetic data. Image segmentation involves classifying each pixel in an image, making preprocessing steps critical for successful training and evaluation of models.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Generate synthetic images and corresponding segmentation masks\n",
    "\n",
    "- Apply common preprocessing techniques such as resizing and normalization.\n",
    "\n",
    "Prepare the data in a format suitable for training deep learning models\n",
    "\n",
    "This notebook is ideal for understanding the preprocessing pipeline in a controlled environment before applying it to real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d067439-0746-4f49-a617-2789399780fb",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a> \n",
    "## <font color=\"orange\"> <b> 2. Setup </b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c1c9f10-a3cb-40a7-a416-9657a8feb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1fe8eb9-147a-4f48-86be-d49d12d8282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c86606-cf73-4482-920a-283e1476bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# for default values\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    batch_size: int = 32\n",
    "    width: int = 224\n",
    "    height: int = 224\n",
    "\n",
    "G = DataConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e4720-e512-495c-9ee9-52715bcdfefe",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a> \n",
    "## <font color=\"orange\"> <b> 3. Synthetic Data Generation for Segmentation </b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ecaaaa0-95fd-4183-bf69-fa5280b13bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output folders\n",
    "image_dir = \"synthetic_dataset/images\"\n",
    "mask_dir = \"synthetic_dataset/masks\"\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "num_samples = 10\n",
    "img_size = 128\n",
    "shape_colors = {\n",
    "    \"square\": (255, 0, 0),  # Red\n",
    "    \"circle\": (0, 255, 0),  # Green\n",
    "}\n",
    "\n",
    "for i in range(num_samples):\n",
    "    img = np.ones((img_size, img_size, 3), dtype=np.uint8) * 255  # white background\n",
    "    mask = np.zeros((img_size, img_size), dtype=np.uint8)         # 0=background (background will be class 0)\n",
    "\n",
    "    shapes = random.choices([\"square\", \"circle\"], k=random.randint(1, 2))  # at least 5 shape\n",
    "\n",
    "    for shape in shapes:\n",
    "        x, y = random.randint(10, 90), random.randint(10, 90)\n",
    "        size = random.randint(15, 30)\n",
    "\n",
    "        if shape == \"square\":\n",
    "            cv2.rectangle(img, (x, y), (x+size, y+size), shape_colors[\"square\"], -1)\n",
    "            cv2.rectangle(mask, (x, y), (x+size, y+size), 1, -1) # square = 1 (square will be class 1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(img, (x, y), size//2, shape_colors[\"circle\"], -1)\n",
    "            cv2.circle(mask, (x, y), size//2, 2, -1) # circle will be class 2\n",
    "\n",
    "    cv2.imwrite(f\"{image_dir}/img_{i:03d}.png\", img)\n",
    "    cv2.imwrite(f\"{mask_dir}/mask_{i:03d}.png\", mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ebd92ac-bed1-4f49-9448-4b477c76fcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x600 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Directories\n",
    "image_dir = \"synthetic_dataset/images\"\n",
    "mask_dir = \"synthetic_dataset/masks\"\n",
    "\n",
    "# Number of samples to plot\n",
    "num_samples_to_show = 6  # Multiple of 3 for even rows\n",
    "\n",
    "# Get sample filenames\n",
    "image_filenames = sorted(os.listdir(image_dir))[:num_samples_to_show]\n",
    "mask_filenames = sorted(os.listdir(mask_dir))[:num_samples_to_show]\n",
    "\n",
    "# Plot settings\n",
    "pairs_per_row = 3\n",
    "fig, axs = plt.subplots(num_samples_to_show // pairs_per_row, pairs_per_row * 2, figsize=(pairs_per_row * 3, (num_samples_to_show // pairs_per_row) * 3))\n",
    "\n",
    "for i, (img_file, mask_file) in enumerate(zip(image_filenames, mask_filenames)):\n",
    "    # Load image and mask\n",
    "    img = cv2.imread(os.path.join(image_dir, img_file))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.imread(os.path.join(mask_dir, mask_file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    row = i // pairs_per_row\n",
    "    col = (i % pairs_per_row) * 2\n",
    "\n",
    "    axs[row, col].imshow(img)\n",
    "    axs[row, col].set_title(\"Image\")\n",
    "    axs[row, col].axis(\"off\")\n",
    "\n",
    "    axs[row, col + 1].imshow(mask, cmap='jet', vmin=0, vmax=2)\n",
    "    axs[row, col + 1].set_title(\"Mask\")\n",
    "    axs[row, col + 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6161d64b-f4ce-4181-9b26-057e141e80a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_000.png  img_002.png  img_004.png  img_006.png  img_008.png\n",
      "img_001.png  img_003.png  img_005.png  img_007.png  img_009.png\n"
     ]
    }
   ],
   "source": [
    "!ls synthetic_dataset/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de76a0a9-fa22-4c55-a804-632737b40ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = \"synthetic_dataset/images\"\n",
    "train_annotation_dir = \"synthetic_dataset/masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "308118e9-74d9-4485-93ce-b92ab4a48cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['background', 'square', 'circle']\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bce3e29-1955-4186-baf8-e582e22d7a6c",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a> \n",
    "## <font color=\"orange\"> <b> 4. Image and Mask Pre-processing </b> </font>\n",
    "\n",
    "<a name=\"4.1\"></a> \n",
    "### <font color=\"#ca6f1e\"> <b> 4.1. Image Pre-processing </b> </font>\n",
    "\n",
    "We will\n",
    "\n",
    "- Read the image\n",
    "- Resize to the desired size.\n",
    "- Normalize pixel values to [-1, 1]\n",
    "\n",
    "We will take one image as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "507d606d-d423-4f44-8395-3d4087e0fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = image_dir + \"/img_007.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ef76dd1-90b5-440d-9dc6-99c56d35f0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAADyElEQVR4nO3dsU0cARBA0TkgdwOEjkkogSos0YMjx+7AxbgFGnATzggQRBay18E1cEKwq73/nrTxTHBfexvNYVmWZYCzdrH1AsDHEzoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0CrrZegP1YluNzTg6H43PuhM7Jfv+e+f595vl5603ex+XlzLdvM7e3W2/y8YTOyV5eZn7+nHl83HqT93F5OXN/v/UW6/CNDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFCh4CrrRdgPz59mvnyZeblZetN3sfFxcz19dZbrOOwLMuy9RLsw7n+Ug6HrTf4eN7onKwQxLnyjQ4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0Cdn3A4elp5uFh5vV1/dmfP8/c3Kw/F95i1yeZfv2aubs7Br+2r19nfvxYfy68xa7f6DMzf//O/Pu3/twtZsJb+UaHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQsOsji9fXx4umf/6sP9vJZPZk12eTgdP46w4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CPgPyTg8XbIu93IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the image\n",
    "img = mpimg.imread(img_path)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off') # Turn off axis labels and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b91a26f-5cb1-4bac-90bc-5c38e9e77c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read image\n",
    "img_raw = tf.io.read_file(img_path)\n",
    "type(img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66425df9-94d4-4d64-bd8d-afe67d9a1e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x02\\x00\\x00\\x00L\\\\\\xf6\\x9c\\x00\\x00\\x02FIDATx\\x01\\xed\\xc11\\n\\xc3@\\x10\\x04\\xb0\\x99\\xff?Z\\xe9\\xdc\\x9b\\xdc\\xb1`V*\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xf2Emn g\\x15\\xf9\\xa267\\x90\\xb3\\x8a|Q\\x9b\\x1b\\xc8YE\\xbe\\xa8\\xcd\\r\\xe4\\xac\"_\\xd4\\xe6\\x06rV\\x91/js\\x039\\xab\\xc8\\x17\\xb5\\xb9\\x81\\x9cU\\xe4\\x8b\\xda\\xdc@\\xce*\\xf2Emn g\\x15\\xf9\\xa267\\x90\\xb3\\x8a|Q\\x9b\\x1b\\xc8YE\\xbe\\xa8\\xcd\\r\\xe4\\xac\"_\\xd4\\xe6\\x06rV\\x91/js\\x039\\xab\\xc8\\x17\\xb5\\xb9\\x81\\x9cU\\xe4\\x8b\\xda\\xdc@\\xce*\\xf2Emn g\\x15\\xf9\\xa267\\x90\\xb3\\x8a|Q\\x9b\\x1b\\xc8YE\\xbe\\xa8\\xcd\\r\\xe4\\xac\"_\\xd4\\xe6\\x06rV\\x91/js\\x039\\xab\\xc8\\x17\\xb5\\xb9\\x81\\x9cU\\xe4\\x8b\\xda\\xdc@\\xce*\\xf2Emn g\\x15\\xf9\\xa267\\x90\\xb3\\x8a|Q\\x9b\\x1b\\xc8YE\\xbe\\xa8\\xcd\\r\\xe4\\xac\"_\\xd4\\xe6\\x06rV\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"\\x7fh\\xf3\\x16Y\\x8f\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9a\\xf3\\x03y\\xab\\xdb\\x10Q\\xa4A\\xd6\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa813def-cb38-4d90-adae-eba9d51dbc88",
   "metadata": {},
   "source": [
    "We can see that the image contains binary data. Let's decode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53a6fd57-d9cc-442d-b326-b3ad9114764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.image.decode_png(img_raw, channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e1979c6-60b6-44e8-bb82-573218f2f89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.framework.ops.EagerTensor,\n",
       " <tf.Tensor: shape=(128, 128, 3), dtype=uint8, numpy=\n",
       " array([[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]], dtype=uint8)>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image), image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe3d46-f469-43f4-b3cc-f4b56b2a7f8a",
   "metadata": {},
   "source": [
    "Now it contains pixel values (between 0 and 255).\n",
    "\n",
    "Now, let's resize the image to the desired width and height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa5adbf4-4df3-4100-a08a-edfef792679c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 128, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6423fcf-51db-4fa4-97e4-26906792079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([224, 224, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width = 224\n",
    "height = 224\n",
    "\n",
    "image = tf.image.resize(image, (height, width))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2239488-611e-4202-817a-0c76fe78bc7f",
   "metadata": {},
   "source": [
    "Finally, let's normalize pixel values to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c9b8deb-b5c7-4c1e-8b20-ebb91cdb4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.cast(image, tf.float32) / 127.5 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e625860-8cef-45cd-805e-a32ee8ff6814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy=\n",
       "array([[[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1268553-a07e-4dd8-be67-b9e684e6c10f",
   "metadata": {},
   "source": [
    "Now the pixel values are between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857393b-8365-48fe-a665-ae05e103f3e9",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a> \n",
    "### <font color=\"#ca6f1e\"> <b> 4.2. Mask Pre-processing </b> </font>\n",
    "\n",
    "It is important to note that the mask is not an image but a label class map.\n",
    "\n",
    "- In image segmentation tasks, the mask file is usually stored as an image (e.g., PNG), but its content is not a regular RGB image.\n",
    "\n",
    "- Instead, each pixel value in the mask represents a class label (e.g., 0 = background, 1 = car, 2 = person, etc.).\n",
    "\n",
    "- So while technically stored as an image, semantically it's a class label map, not visual data.\n",
    "\n",
    "\n",
    "We will:\n",
    "\n",
    "- Read the mask.\n",
    "- Resize to the desire size.\n",
    "- Apply One-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5b3dbdc-3128-4383-bc1d-4bde6575d7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEHElEQVR4nO3dsYpcdRyG4f/s7G4WkmBSpBCCQkiC1mJrJ3gBpreTNHZegVgGwTZYpsoFpLAQi7RBS0HCNgYsUglq2J09XoLLsHuO4/s89cDva17OOdWspmmaBvC/trf0AODyCR0ChA4BQocAoUOA0CFA6BAgdAgQOgTsn/eHH+89uMwdwJa+P3v6r7/xRIcAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHgP2lB7BjVqulF1ysaVp6wSyEzrmt790ZL7++Om5e/3PpKRfidLMebz26NtY/vFh6yqUTOud2duPqePLhd+ODK4dLT7kQb6aT8dHtL8aNpYfMwDc6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAvaXHsDuWL/+Yzz48eE4uvZm6SkXYrPZG+8c/730jFkInXM7fXk87n12vPQMtuDVHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQJ2+p9a1rdujdef3B1nB/PfvvnLX2P1/Kf5D8MWdjr0k/duj8dffTPeP5i/9PvPPh/3n89+Fray06GPvTGOVptxsDqa//Zq/pOwLd/oECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAnb6TxYPf/19fPrtl2NzOP/td38+mf8obGmnQz/97dV4+9GrpWfAf55XdwgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgGraZqmpUcAl8sTHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAv4BjWA/6tC7Rc4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask_path = mask_dir + \"/mask_007.png\"\n",
    "\n",
    "# plot the image\n",
    "mask = mpimg.imread(mask_path)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "\n",
    "plt.imshow(mask)\n",
    "plt.axis('off') # Turn off axis labels and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80c1d43a-1a4b-496e-86e5-aafa2e9c6f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x00\\x00\\x00\\x00\\xe6U>\\x17\\x00\\x00\\x01mIDATx\\x01\\xc5\\xc11\\x0e\\x00 \\x0c\\xc4\\xb0\\xe4\\xff\\x8f.;\\xa2\\x9d\\x8a\\xce\\x960\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x93%2(Z\\xb2D\\x06EK\\x96\\xc8\\xa0h\\xc9\\x12\\x19\\x14-Y\"\\x83\\xa2%KdP\\xb4d\\x89\\x0c\\x8a\\x96,\\x91A\\xd1\\x92%2(Z\\xb2D\\x06EK\\x96\\xc8\\xa0h\\xc9\\x12\\x19\\x14-Y\"\\x83\\xa2%KdP\\xb4d\\x89\\x0c\\x8a\\x96,\\x91A\\xd1\\x92%2(Z\\xb2D\\x06EK\\x96\\xc8\\xa0h\\xc9\\x12\\x19\\x14-Y\"\\x83\\xa2%KdP\\xb4d\\x89\\x0c\\x8a\\x96,\\x91A\\xd1\\x92%2(Z\\xb2D\\x06EK\\x96\\xc8\\xa0h\\xc9\\x12\\x19\\x14-Y\"\\x83\\xa2%a\\x12&a\\x12&a\\x12&a\\x12&a\\x12&a\\x12&a\\x12&a\\xf2\"\\x97\\xe2\\x17y\\x91K\\xf1\\x8b\\xbc\\xc8\\xa5\\xf8E^\\xe4R\\xfc\"/r)~\\x91\\x17\\xb9\\x14\\xbf\\xc8\\x8b\\\\\\x8a_\\xe4E.\\xc5/\\xf2\"\\x97\\xe2\\x17y\\x91K\\xf1\\x8b\\xbc\\xc8\\xa5\\xf8E^\\xe4R\\xfc\"/r)~\\x91\\x17\\xb9\\x14\\xbf\\xc8\\x8b\\\\\\x8a_\\xe4E.\\xc5/\\xf2\"\\x97\\xe2\\x17\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t\\x930\\t;mw.\\x81:pk\\xdd\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the annotation\n",
    "mask_raw = tf.io.read_file(mask_path)\n",
    "mask_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70843145-e887-45dd-aef5-8fef749ee901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 128, 1), dtype=uint8, numpy=\n",
       "array([[[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]],\n",
       "\n",
       "       [[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]], dtype=uint8)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode\n",
    "mask = tf.image.decode_png(mask_raw, channels=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f13c37-d671-4f8e-ba0c-54baeb0a90e4",
   "metadata": {},
   "source": [
    "It contains integer values between 0 and 1 (0,1 and 2) representing the 3 different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a482f8de-a86a-49fd-8d19-b35f472330d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 128, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77668bde-24e5-445f-b2ce-e18381174905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([224, 224, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize\n",
    "mask = tf.image.resize(mask, (height, width), method='nearest')  # Keep class labels intact\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cefb3550-1462-4462-a326-eaef70e8a14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int32"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast\n",
    "mask = tf.cast(mask, tf.int32)\n",
    "mask.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5696daa1-00a4-44ef-8ba3-643f3ceb8692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([224, 224])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove channel dimension (H, W, 1) -> (H, W)\n",
    "mask = tf.squeeze(mask, axis=-1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ebf172c9-d239-4651-9848-ec97fb3d62e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([224, 224, 3]),\n",
       " <tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy=\n",
       " array([[[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]],\n",
       " \n",
       "        [[1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.]]], dtype=float32)>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert annotation to one-hot format: shape becomes (H, W, num_classes)\n",
    "mask = tf.one_hot(mask, depth=num_classes)\n",
    "mask.shape, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea37ca2-1cb7-4fd5-81ad-474d715d8e1e",
   "metadata": {},
   "source": [
    "<a name=\"4.3\"></a> \n",
    "### <font color=\"#ca6f1e\"> <b> 4.3. Putting everything together </b> </font>\n",
    "\n",
    "We can combine image and mask pre-processing in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "100afb07-f503-4960-ba5f-d9af2f4d1ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_filename_to_image_and_mask(t_filename, a_filename, height=64, width=64, num_classes=num_classes):\n",
    "    \"\"\"\n",
    "    Reads and preprocesses an image and its corresponding segmentation mask from given file paths.\n",
    "\n",
    "    Args:\n",
    "        t_filename (tf.Tensor or str): Path to the input image file.\n",
    "        a_filename (tf.Tensor or str): Path to the corresponding annotation (mask) file.\n",
    "        height (int): Target height to resize the image and mask. Defaults to G.height.\n",
    "        width (int): Target width to resize the image and mask. Defaults to G.width.\n",
    "        num_classes (int): Number of segmentation classes for one-hot encoding. Defaults to num_classes.\n",
    "\n",
    "    Returns:\n",
    "        image (tf.Tensor): Preprocessed image tensor of shape (height, width, 3), normalized to [-1, 1].\n",
    "        annotation (tf.Tensor): One-hot encoded mask tensor of shape (height, width, num_classes).\n",
    "    \"\"\"\n",
    "\n",
    "    # Read and decode the image (3 channels)\n",
    "    img_raw = tf.io.read_file(t_filename)\n",
    "    image = tf.image.decode_png(img_raw, channels=3)\n",
    "    image = tf.image.resize(image, (height, width))  # Resize image to desired size\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1.0  # Normalize pixel values to [-1, 1]\n",
    "\n",
    "    # Read and decode the annotation mask (1 channel)\n",
    "    anno_raw = tf.io.read_file(a_filename)\n",
    "    annotation = tf.image.decode_png(anno_raw, channels=1)\n",
    "    annotation = tf.image.resize(annotation, (height, width), method='nearest')  # Keep class labels intact\n",
    "    annotation = tf.cast(annotation, tf.int32)\n",
    "    annotation = tf.squeeze(annotation, axis=-1)  # Remove channel dimension (H, W, 1) -> (H, W)\n",
    "\n",
    "    # Convert annotation to one-hot format: shape becomes (H, W, num_classes)\n",
    "    annotation = tf.one_hot(annotation, depth=num_classes)\n",
    "\n",
    "    return image, annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ce88c-22e9-47fe-9e93-3275fd98303e",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a> \n",
    "## <font color=\"orange\"> <b> 5. Creating TF Datasets </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319dd56a-2bec-489a-ad39-666f673e5ef0",
   "metadata": {},
   "source": [
    "First, we need a function that returns sorted file paths for images and their corresponding segmentation masks (label maps). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "101073b0-6f56-48d8-a343-10639286d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_slice_paths(image_dir, label_map_dir):\n",
    "    '''\n",
    "    Returns sorted paths to image and mask files.\n",
    "    '''\n",
    "    image_file_list = sorted(os.listdir(image_dir))\n",
    "    label_map_file_list = sorted(os.listdir(label_map_dir))\n",
    "\n",
    "    image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
    "    label_map_paths = [os.path.join(label_map_dir, fname) for fname in label_map_file_list]\n",
    "\n",
    "    return image_paths, label_map_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d856430e-9b7e-4348-ba6a-b8f85b2a4db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['synthetic_dataset/images/img_000.png',\n",
       "  'synthetic_dataset/images/img_001.png',\n",
       "  'synthetic_dataset/images/img_002.png',\n",
       "  'synthetic_dataset/images/img_003.png',\n",
       "  'synthetic_dataset/images/img_004.png',\n",
       "  'synthetic_dataset/images/img_005.png',\n",
       "  'synthetic_dataset/images/img_006.png',\n",
       "  'synthetic_dataset/images/img_007.png',\n",
       "  'synthetic_dataset/images/img_008.png',\n",
       "  'synthetic_dataset/images/img_009.png'],\n",
       " ['synthetic_dataset/masks/mask_000.png',\n",
       "  'synthetic_dataset/masks/mask_001.png',\n",
       "  'synthetic_dataset/masks/mask_002.png',\n",
       "  'synthetic_dataset/masks/mask_003.png',\n",
       "  'synthetic_dataset/masks/mask_004.png',\n",
       "  'synthetic_dataset/masks/mask_005.png',\n",
       "  'synthetic_dataset/masks/mask_006.png',\n",
       "  'synthetic_dataset/masks/mask_007.png',\n",
       "  'synthetic_dataset/masks/mask_008.png',\n",
       "  'synthetic_dataset/masks/mask_009.png'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it\n",
    "get_dataset_slice_paths(image_dir, mask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e35d5-50b6-4e1f-80f9-960512c91cf6",
   "metadata": {},
   "source": [
    "Finally, we will prepare the TF Datasets.\n",
    "\n",
    "We will:\n",
    "\n",
    "-  Creates a TensorFlow dataset from the given lists of file paths.\n",
    "-  Apply the pre-processing function to every element in the dataset.\n",
    "-  Shuffle data (only in training)\n",
    "-  Configure batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "acd5605f-6b97-4641-a47c-12fc398850e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Path: b'synthetic_dataset/images/img_000.png'\n",
      "Mask Path: b'synthetic_dataset/masks/mask_000.png'\n",
      "Image Path: b'synthetic_dataset/images/img_001.png'\n",
      "Mask Path: b'synthetic_dataset/masks/mask_001.png'\n"
     ]
    }
   ],
   "source": [
    "# sorted file paths\n",
    "training_image_paths, training_label_map_paths = get_dataset_slice_paths(image_dir, mask_dir)\n",
    "\n",
    "# create TF dataset\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((training_image_paths, training_label_map_paths ))\n",
    "\n",
    "# Now each element of training_dataset will be a tuple of two tensors (image_path, mask_path)\n",
    "for image_path_tensor, mask_path_tensor in training_dataset.take(2):\n",
    "    print(f\"Image Path: {image_path_tensor.numpy()}\")\n",
    "    print(f\"Mask Path: {mask_path_tensor.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50a34a02-dbce-4c15-aa8e-f205db22e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: [[[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]]\n",
      "Mask: [[[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# apply the pre-processing function to every element in the dataset\n",
    "training_dataset = training_dataset.map(map_filename_to_image_and_mask)\n",
    "\n",
    "for image_path_tensor, mask_path_tensor in training_dataset.take(1):\n",
    "    print(f\"Image: {image_path_tensor.numpy()}\")\n",
    "    print(f\"Mask: {mask_path_tensor.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b916696-013b-43c3-b632-de30a9d0798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle (only for training)\n",
    "training_dataset = training_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
    "\n",
    "# configure batch size\n",
    "training_dataset = training_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f44144-b770-477a-98a1-f7d63fee9d39",
   "metadata": {},
   "source": [
    "Finally, we can combine this in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d23f476e-2d57-4c29-b996-ca6b55483cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(image_paths, label_map_paths, batch_size = 8, training=True):\n",
    "  '''\n",
    "    Args:\n",
    "        image_paths (list of str): Paths to each image file in the dataset.\n",
    "        label_map_paths (list of str): Paths to each label map file.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        training (bool): Whether the dataset is for training (enables shuffling).\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: A dataset yielding batches of preprocessed images and masks.\n",
    "  '''\n",
    "  training_dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
    "  training_dataset = training_dataset.map(map_filename_to_image_and_mask)\n",
    "  if training:\n",
    "      training_dataset = training_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
    "  training_dataset = training_dataset.batch(batch_size)\n",
    "\n",
    "  return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6fa0af3-8c02-416b-aa5a-fdb7767dc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths to the images\n",
    "training_image_paths, training_label_map_paths = get_dataset_slice_paths(train_img_dir, train_annotation_dir)\n",
    "\n",
    "# generate the train and val sets\n",
    "training_dataset = get_dataset(training_image_paths, training_label_map_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bce4503-9e43-4b86-a2f8-a31fbd2e0265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: (8, 64, 64, 3)\n",
      "Mask: (8, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, mask_batch in training_dataset.take(1):\n",
    "    print(f\"Image: {image_batch.shape}\") # batch size, width, height, number of channels\n",
    "    print(f\"Mask: {mask_batch.shape}\") # batch size, , width, height, num classes (one-hot encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc8e4f-bd71-4a53-b21d-15db173c0f89",
   "metadata": {},
   "source": [
    "<a name=\"references\"></a> \n",
    "## <font color=\"orange\"> <b> References </b> </font>\n",
    "\n",
    "[TF Advanced Techniques Specialization](https://www.coursera.org/specializations/tensorflow-advanced-techniques)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
