{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d545d3c7-39a5-46f8-a165-b25a1e23947b",
   "metadata": {},
   "source": [
    "# <center><div style=\"color:red\">PRE-PROCESSING: SIMPLE DATA</div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778ebcc-e85b-47ea-9161-8fa2347e5b34",
   "metadata": {},
   "source": [
    "### <font color='blue'> Table of Contents </font>\n",
    "- [1 - Objectives](#1)\n",
    "- [2 - Setup](#2)\n",
    "- [3 - Helper Functions](#3)\n",
    "- [4 - Data](#4) <br>\n",
    "    - [4.1. - Generating Synthetic Data](#4.1)\n",
    "    - [4.2. - Pre-procesing](#4.2)\n",
    "- [5 - Build, compile and train the model](#5)\n",
    "- [6 - Predictions](#6)\n",
    "- [7 - Evaluation](#7)\n",
    "- [8 - Training with Dice Loss](#8)\n",
    "- [9 - References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d66ecc-0f57-4bf0-9eff-e0576cfb4bbe",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a> \n",
    "## <font color=\"orange\"> <b> 1. Introduction </b> </font>\n",
    "\n",
    "In this notebook, we demonstrate how to perform preprocessing for image segmentation tasks using simple synthetic data. Image segmentation involves classifying each pixel in an image, making preprocessing steps critical for successful training and evaluation of models.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Generate synthetic images and corresponding segmentation masks\n",
    "\n",
    "- Apply common preprocessing techniques such as resizing and normalization.\n",
    "\n",
    "Prepare the data in a format suitable for training deep learning models\n",
    "\n",
    "This notebook is ideal for understanding the preprocessing pipeline in a controlled environment before applying it to real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d067439-0746-4f49-a617-2789399780fb",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a> \n",
    "## <font color=\"orange\"> <b> 2. Setup </b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8c1c9f10-a3cb-40a7-a416-9657a8feb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e1fe8eb9-147a-4f48-86be-d49d12d8282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f3c86606-cf73-4482-920a-283e1476bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# for default values\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    batch_size: int = 32\n",
    "    width: int = 224\n",
    "    height: int = 224\n",
    "\n",
    "G = DataConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e4720-e512-495c-9ee9-52715bcdfefe",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a> \n",
    "## <font color=\"orange\"> <b> 3. Synthetic Data Generation for Segmentation </b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2ecaaaa0-95fd-4183-bf69-fa5280b13bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output folders\n",
    "image_dir = \"synthetic_dataset/images\"\n",
    "mask_dir = \"synthetic_dataset/masks\"\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "num_samples = 10\n",
    "img_size = 128\n",
    "shape_colors = {\n",
    "    \"square\": (255, 0, 0),  # Red\n",
    "    \"circle\": (0, 255, 0),  # Green\n",
    "}\n",
    "\n",
    "for i in range(num_samples):\n",
    "    img = np.ones((img_size, img_size, 3), dtype=np.uint8) * 255  # white background\n",
    "    mask = np.zeros((img_size, img_size), dtype=np.uint8)         # 0=background\n",
    "\n",
    "    shapes = random.choices([\"square\", \"circle\"], k=random.randint(1, 2))  # at least 5 shape\n",
    "\n",
    "    for shape in shapes:\n",
    "        x, y = random.randint(10, 90), random.randint(10, 90)\n",
    "        size = random.randint(15, 30)\n",
    "\n",
    "        if shape == \"square\":\n",
    "            cv2.rectangle(img, (x, y), (x+size, y+size), shape_colors[\"square\"], -1)\n",
    "            cv2.rectangle(mask, (x, y), (x+size, y+size), 1, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(img, (x, y), size//2, shape_colors[\"circle\"], -1)\n",
    "            cv2.circle(mask, (x, y), size//2, 2, -1)\n",
    "\n",
    "    cv2.imwrite(f\"{image_dir}/img_{i:03d}.png\", img)\n",
    "    cv2.imwrite(f\"{mask_dir}/mask_{i:03d}.png\", mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6ebd92ac-bed1-4f49-9448-4b477c76fcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x600 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Directories\n",
    "image_dir = \"synthetic_dataset/images\"\n",
    "mask_dir = \"synthetic_dataset/masks\"\n",
    "\n",
    "# Number of samples to plot\n",
    "num_samples_to_show = 6  # Multiple of 3 for even rows\n",
    "\n",
    "# Get sample filenames\n",
    "image_filenames = sorted(os.listdir(image_dir))[:num_samples_to_show]\n",
    "mask_filenames = sorted(os.listdir(mask_dir))[:num_samples_to_show]\n",
    "\n",
    "# Plot settings\n",
    "pairs_per_row = 3\n",
    "fig, axs = plt.subplots(num_samples_to_show // pairs_per_row, pairs_per_row * 2, figsize=(pairs_per_row * 3, (num_samples_to_show // pairs_per_row) * 3))\n",
    "\n",
    "for i, (img_file, mask_file) in enumerate(zip(image_filenames, mask_filenames)):\n",
    "    # Load image and mask\n",
    "    img = cv2.imread(os.path.join(image_dir, img_file))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.imread(os.path.join(mask_dir, mask_file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    row = i // pairs_per_row\n",
    "    col = (i % pairs_per_row) * 2\n",
    "\n",
    "    axs[row, col].imshow(img)\n",
    "    axs[row, col].set_title(\"Image\")\n",
    "    axs[row, col].axis(\"off\")\n",
    "\n",
    "    axs[row, col + 1].imshow(mask, cmap='jet', vmin=0, vmax=2)\n",
    "    axs[row, col + 1].set_title(\"Mask\")\n",
    "    axs[row, col + 1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6161d64b-f4ce-4181-9b26-057e141e80a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_000.png  img_002.png  img_004.png  img_006.png  img_008.png\n",
      "img_001.png  img_003.png  img_005.png  img_007.png  img_009.png\n"
     ]
    }
   ],
   "source": [
    "!ls synthetic_dataset/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "de76a0a9-fa22-4c55-a804-632737b40ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = \"synthetic_dataset/images\"\n",
    "train_annotation_dir = \"synthetic_dataset/masks\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bce3e29-1955-4186-baf8-e582e22d7a6c",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a> \n",
    "## <font color=\"orange\"> <b> 4. Image and Mask Pre-processing </b> </font>\n",
    "\n",
    "<a name=\"4.1\"></a> \n",
    "### <font color=\"#ca6f1e\"> <b> 4.1. Image Pre-processing </b> </font>\n",
    "\n",
    "We will\n",
    "\n",
    "- Read the image\n",
    "- Resize to the desired size.\n",
    "- Normalize pixel values to [-1, 1]\n",
    "\n",
    "We will take one image as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "507d606d-d423-4f44-8395-3d4087e0fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = image_dir + \"/img_007.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9ef76dd1-90b5-440d-9dc6-99c56d35f0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAADuElEQVR4nO3cIU4cYRyH4T8NBskFwKK5QDXBV3IiDIYLYDgLd6BBkmAwKJJOBamrWNiUYfs+T7Jyv/zMO5Mx396yLMsA/7Vvaw8A/j2hQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoELC/9gD44/l55uVl7RWbOTiYOTyc2dtbe8lmhM6XsCwzl5czt7drL9nM2dnM1ZXQ4d2enmbu79desZnHx7eH067wjQ4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgDvj2MqyzNzdzTw8bH/Wz5/bn8HfCZ2tXV/P3Nxsf84uXba4a4TO1pZFpF+db3QIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAtwZR9KPHzPn5x////HxzLcdek0KnaTT05mLi7VXfJ4deiYBHyV0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFugWVrJycz37+vveJ9jo7WXvC59pZlWdYewe5alpnX15lfv9Ze8j77+2+/CqFDgG90CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQ8BvOcDictYwS1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the image\n",
    "img = mpimg.imread(img_path)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off') # Turn off axis labels and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0b91a26f-5cb1-4bac-90bc-5c38e9e77c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read image\n",
    "img_raw = tf.io.read_file(img_path)\n",
    "type(img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "66425df9-94d4-4d64-bd8d-afe67d9a1e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x08\\x02\\x00\\x00\\x00L\\\\\\xf6\\x9c\\x00\\x00\\x02\\x1bIDATx\\x01\\xed\\xc1\\x81\\t\\x041\\x0c\\x030{\\xff\\xa1\\xf5;\\x94\\xf2\\x81^\\xa4\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aS\\xe4\\xc3\\xda\\\\A\\xce\\x14\\xf9\\xb06W\\x903E>\\xac\\xcd\\x15\\xe4L\\x91\\x0fks\\x059S\\xe4\\xc3\\xda\\\\A\\xce\\x14\\xf9\\xb06W\\x903E>\\xac\\xcd\\x15\\xe4L\\x91\\x0fks\\x059S\\xe4\\xc3\\xda\\\\A\\xce\\x14\\xf9\\xb06W\\x903E>\\xac\\xcd\\x15\\xe4L\\x91\\x0fks\\x059S\\xe4im\\xfe\\x80\\x9c)\\xf2\\xb46\\x7f@\\xce\\x14yZ\\x9b? g\\x8a<\\xad\\xcd\\x1f\\x903E\\x9e\\xd6\\xe6\\x0f\\xc8\\x99\"Ok\\xf3\\x07\\xe4L\\x91\\xa7\\xb5\\xf9\\x03r\\xa6\\xc8\\xd3\\xda\\xfc\\x019S\\xe4im\\xfe\\x80\\x9c)\\xf2\\xb46\\x7f@\\xce\\x14yZ\\x9b? g\\x8a<\\xad\\xcd\\x1f\\x903E\\x9e\\xd6\\xe6\\x16r]\\x91\\xa7\\xb5\\xb9\\x85\\\\W\\xe4imn!\\xd7\\x15yZ\\x9b[\\xc8uE\\x9e\\xd6\\xe6\\x16r]\\x91\\xa7\\xb5\\xb9\\x85\\\\W\\xe4imn!\\xd7\\x15yZ\\x9b[\\xc8uE\\x9e\\xd6\\xe6\\x16r]\\x91\\xa7\\xb5\\xb9\\x85\\\\W\\xe4imn!\\xd7\\x15yZ\\x9b[\\xc8uE\\x9e\\xd6\\xe6\\x16r]\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"kN\\x915\\xa7\\xc8\\x9aSd\\xcd)\\xb2\\xe6\\x14Ys\\x8a\\xac9E\\xd6\\x9c\"k\\xce\\x0fn\\x8d\\xc9\\x10\\xfb8\\xc6\\xf2\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82'>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa813def-cb38-4d90-adae-eba9d51dbc88",
   "metadata": {},
   "source": [
    "We can see that the image contains binary data. Let's decode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "53a6fd57-d9cc-442d-b326-b3ad9114764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.image.decode_png(img_raw, channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3e1979c6-60b6-44e8-bb82-573218f2f89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.framework.ops.EagerTensor,\n",
       " <tf.Tensor: shape=(128, 128, 3), dtype=uint8, numpy=\n",
       " array([[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]], dtype=uint8)>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image), image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe3d46-f469-43f4-b3cc-f4b56b2a7f8a",
   "metadata": {},
   "source": [
    "Now it contains pixel values (between 0 and 255).\n",
    "\n",
    "Now, let's resize the image to the desired width and height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "fa5adbf4-4df3-4100-a08a-edfef792679c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 128, 3])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c6423fcf-51db-4fa4-97e4-26906792079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([224, 224, 3])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width = 224\n",
    "height = 224\n",
    "\n",
    "image = tf.image.resize(image, (height, width))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2239488-611e-4202-817a-0c76fe78bc7f",
   "metadata": {},
   "source": [
    "Finally, let's normalize pixel values to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1c9b8deb-b5c7-4c1e-8b20-ebb91cdb4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.cast(image, tf.float32) / 127.5 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7e625860-8cef-45cd-805e-a32ee8ff6814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy=\n",
       "array([[[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1268553-a07e-4dd8-be67-b9e684e6c10f",
   "metadata": {},
   "source": [
    "Now the pixel values are between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857393b-8365-48fe-a665-ae05e103f3e9",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a> \n",
    "### <font color=\"#ca6f1e\"> <b> 4.2. Mask Pre-processing </b> </font>\n",
    "\n",
    "It is important to note that the mask is not an image but a label class map.\n",
    "\n",
    "- In image segmentation tasks, the mask file is usually stored as an image (e.g., PNG), but its content is not a regular RGB image.\n",
    "\n",
    "- Instead, each pixel value in the mask represents a class label (e.g., 0 = background, 1 = car, 2 = person, etc.).\n",
    "\n",
    "- So while technically stored as an image, semantically it's a class label map, not visual data.\n",
    "\n",
    "\n",
    "We will:\n",
    "\n",
    "- Read the mask.\n",
    "- Resize to the desire size.\n",
    "- Apply One-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b9ce0-3795-4ab9-a423-9a5dc2bf3e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c1d43a-1a4b-496e-86e5-aafa2e9c6f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482f8de-a86a-49fd-8d19-b35f472330d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77668bde-24e5-445f-b2ce-e18381174905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb3550-1462-4462-a326-eaef70e8a14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ea37ca2-1cb7-4fd5-81ad-474d715d8e1e",
   "metadata": {},
   "source": [
    "<a name=\"4.3\"></a> \n",
    "### <font color=\"#ca6f1e\"> <b> 4.3. Putting everything together </b> </font>\n",
    "\n",
    "We can combine image and mask pre-processing in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "100afb07-f503-4960-ba5f-d9af2f4d1ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_filename_to_image_and_mask(t_filename, a_filename, height=64, width=64, num_classes=3):\n",
    "    \"\"\"\n",
    "    Reads and preprocesses an image and its corresponding segmentation mask from given file paths.\n",
    "\n",
    "    Args:\n",
    "        t_filename (tf.Tensor or str): Path to the input image file.\n",
    "        a_filename (tf.Tensor or str): Path to the corresponding annotation (mask) file.\n",
    "        height (int): Target height to resize the image and mask. Defaults to G.height.\n",
    "        width (int): Target width to resize the image and mask. Defaults to G.width.\n",
    "        num_classes (int): Number of segmentation classes for one-hot encoding. Defaults to num_classes.\n",
    "\n",
    "    Returns:\n",
    "        image (tf.Tensor): Preprocessed image tensor of shape (height, width, 3), normalized to [-1, 1].\n",
    "        annotation (tf.Tensor): One-hot encoded mask tensor of shape (height, width, num_classes).\n",
    "    \"\"\"\n",
    "\n",
    "    # Read and decode the image (3 channels)\n",
    "    img_raw = tf.io.read_file(t_filename)\n",
    "    image = tf.image.decode_png(img_raw, channels=3)\n",
    "    image = tf.image.resize(image, (height, width))  # Resize image to desired size\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1.0  # Normalize pixel values to [-1, 1]\n",
    "\n",
    "    # Read and decode the annotation mask (1 channel)\n",
    "    anno_raw = tf.io.read_file(a_filename)\n",
    "    annotation = tf.image.decode_png(anno_raw, channels=1)\n",
    "    annotation = tf.image.resize(annotation, (height, width), method='nearest')  # Keep class labels intact\n",
    "    annotation = tf.cast(annotation, tf.int32)\n",
    "    annotation = tf.squeeze(annotation, axis=-1)  # Remove channel dimension (H, W, 1) -> (H, W)\n",
    "\n",
    "    # Convert annotation to one-hot format: shape becomes (H, W, num_classes)\n",
    "    annotation = tf.one_hot(annotation, depth=num_classes)\n",
    "\n",
    "    return image, annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ce88c-22e9-47fe-9e93-3275fd98303e",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a> \n",
    "## <font color=\"orange\"> <b> 5. Creating TF Datasets </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319dd56a-2bec-489a-ad39-666f673e5ef0",
   "metadata": {},
   "source": [
    "First, we need a function that returns sorted file paths for images and their corresponding segmentation masks (label maps). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "101073b0-6f56-48d8-a343-10639286d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_slice_paths(image_dir, label_map_dir):\n",
    "    '''\n",
    "    Returns sorted paths to image and mask files.\n",
    "    '''\n",
    "    image_file_list = sorted(os.listdir(image_dir))\n",
    "    label_map_file_list = sorted(os.listdir(label_map_dir))\n",
    "\n",
    "    image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
    "    label_map_paths = [os.path.join(label_map_dir, fname) for fname in label_map_file_list]\n",
    "\n",
    "    return image_paths, label_map_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d856430e-9b7e-4348-ba6a-b8f85b2a4db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['synthetic_dataset/images/img_000.png',\n",
       "  'synthetic_dataset/images/img_001.png',\n",
       "  'synthetic_dataset/images/img_002.png',\n",
       "  'synthetic_dataset/images/img_003.png',\n",
       "  'synthetic_dataset/images/img_004.png',\n",
       "  'synthetic_dataset/images/img_005.png',\n",
       "  'synthetic_dataset/images/img_006.png',\n",
       "  'synthetic_dataset/images/img_007.png',\n",
       "  'synthetic_dataset/images/img_008.png',\n",
       "  'synthetic_dataset/images/img_009.png'],\n",
       " ['synthetic_dataset/masks/mask_000.png',\n",
       "  'synthetic_dataset/masks/mask_001.png',\n",
       "  'synthetic_dataset/masks/mask_002.png',\n",
       "  'synthetic_dataset/masks/mask_003.png',\n",
       "  'synthetic_dataset/masks/mask_004.png',\n",
       "  'synthetic_dataset/masks/mask_005.png',\n",
       "  'synthetic_dataset/masks/mask_006.png',\n",
       "  'synthetic_dataset/masks/mask_007.png',\n",
       "  'synthetic_dataset/masks/mask_008.png',\n",
       "  'synthetic_dataset/masks/mask_009.png'])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it\n",
    "get_dataset_slice_paths(image_dir, mask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e35d5-50b6-4e1f-80f9-960512c91cf6",
   "metadata": {},
   "source": [
    "Finally, we will prepare the TF Datasets.\n",
    "\n",
    "We will:\n",
    "\n",
    "-  Creates a TensorFlow dataset from the given lists of file paths.\n",
    "-  Apply the pre-processing function to every element in the dataset.\n",
    "-  Shuffle data (only in training)\n",
    "-  Configure batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "acd5605f-6b97-4641-a47c-12fc398850e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Path: b'synthetic_dataset/images/img_000.png'\n",
      "Mask Path: b'synthetic_dataset/masks/mask_000.png'\n",
      "Image Path: b'synthetic_dataset/images/img_001.png'\n",
      "Mask Path: b'synthetic_dataset/masks/mask_001.png'\n"
     ]
    }
   ],
   "source": [
    "# sorted file paths\n",
    "training_image_paths, training_label_map_paths = get_dataset_slice_paths(image_dir, mask_dir)\n",
    "\n",
    "# create TF dataset\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((training_image_paths, training_label_map_paths ))\n",
    "\n",
    "# Now each element of training_dataset will be a tuple of two tensors (image_path, mask_path)\n",
    "for image_path_tensor, mask_path_tensor in training_dataset.take(2):\n",
    "    print(f\"Image Path: {image_path_tensor.numpy()}\")\n",
    "    print(f\"Mask Path: {mask_path_tensor.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "50a34a02-dbce-4c15-aa8e-f205db22e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: [[[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]]\n",
      "Mask: [[[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# apply the pre-processing function to every element in the dataset\n",
    "training_dataset = training_dataset.map(map_filename_to_image_and_mask)\n",
    "\n",
    "for image_path_tensor, mask_path_tensor in training_dataset.take(1):\n",
    "    print(f\"Image: {image_path_tensor.numpy()}\")\n",
    "    print(f\"Mask: {mask_path_tensor.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6b916696-013b-43c3-b632-de30a9d0798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle (only for training)\n",
    "training_dataset = training_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
    "\n",
    "# configure batch size\n",
    "training_dataset = training_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f44144-b770-477a-98a1-f7d63fee9d39",
   "metadata": {},
   "source": [
    "Finally, we can combine this in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d23f476e-2d57-4c29-b996-ca6b55483cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(image_paths, label_map_paths, batch_size = 8, training=True):\n",
    "  '''\n",
    "    Args:\n",
    "        image_paths (list of str): Paths to each image file in the dataset.\n",
    "        label_map_paths (list of str): Paths to each label map file.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        training (bool): Whether the dataset is for training (enables shuffling).\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: A dataset yielding batches of preprocessed images and masks.\n",
    "  '''\n",
    "  training_dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
    "  training_dataset = training_dataset.map(map_filename_to_image_and_mask)\n",
    "  if training:\n",
    "      training_dataset = training_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
    "  training_dataset = training_dataset.batch(batch_size)\n",
    "\n",
    "  return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c6fa0af3-8c02-416b-aa5a-fdb7767dc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths to the images\n",
    "training_image_paths, training_label_map_paths = get_dataset_slice_paths(train_img_dir, train_annotation_dir)\n",
    "\n",
    "# generate the train and val sets\n",
    "training_dataset = get_dataset(training_image_paths, training_label_map_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2bce4503-9e43-4b86-a2f8-a31fbd2e0265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: (8, 64, 64, 3)\n",
      "Mask: (8, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "for image_batch, mask_batch in training_dataset.take(1):\n",
    "    print(f\"Image: {image_batch.shape}\") # batch size, width, height, number of channels\n",
    "    print(f\"Mask: {mask_batch.shape}\") # batch size, , width, height, num classes (one-hot encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc8e4f-bd71-4a53-b21d-15db173c0f89",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
